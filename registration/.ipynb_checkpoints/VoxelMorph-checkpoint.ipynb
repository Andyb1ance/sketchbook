{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6229f7da",
   "metadata": {},
   "source": [
    "# VoxelMorph\n",
    "\n",
    "无监督的深度学习方法\n",
    "\n",
    "## 网络结构\n",
    "输入Moving Image和Fixed Image两张图，经过一个UNet结构，输出一个Registration Field，此时有一个形变场的平滑损失L_smooth\n",
    "\n",
    "Registration Field和Moving Image输入STN层，输出一个Moved Image，此时Moved Image和Fixed Image有一个相似度损失L_sim\n",
    "\n",
    "如果有分割标签，则将Fixed Seg和Moving Seg以及Registration Field输入STN层，输出一个Moved Seg，此时Fixed Seg和Moved Seg有一个分割损失L_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6bda0",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "用到的UNet: \n",
    "卷积一次，卷积后最大池化\n",
    "input:(1,2,128,128,128) batchsize 1，channel 2（fixed和moving）shape暂定128\n",
    "encoder：1-16-32-32-32-32\n",
    "decoder：32-32-32-32\n",
    "output: 16-16-3-（128,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb7f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "def default_unet_features():\n",
    "    nb_features = [\n",
    "        [16, 32, 32, 32],             # encoder\n",
    "        [32, 32, 32, 32, 32, 16, 16]  # decoder\n",
    "    ]\n",
    "    return nb_features\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Specific convolutional block followed by leakyrelu for unet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndims, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        Conv = getattr(nn, 'Conv%dd' % ndims)\n",
    "        self.main = Conv(in_channels, out_channels, 3, stride, 1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "def default_unet_features():\n",
    "    net_channels = [\n",
    "        [16, 32, 32, 32],             # encoder\n",
    "        [32, 32, 32, 32, 32, 16, 16]  # decoder\n",
    "    ]\n",
    "    return net_channels\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Specific convolutional block followed by leakyrelu for unet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndims, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        Conv = getattr(nn, 'Conv%dd' % ndims)\n",
    "        self.main = Conv(in_channels, out_channels, 3, stride, 1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    A unet architecture. Layer features can be specified directly as a list of encoder and decoder\n",
    "    features or as a single integer along with a number of unet levels. The default network features\n",
    "    per layer (when no options are specified) are:\n",
    "\n",
    "        encoder: [16, 32, 32, 32]\n",
    "        decoder: [32, 32, 32, 32, 32, 16, 16]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 shape=None,\n",
    "                 in_channels=None,\n",
    "                 net_channels=None,\n",
    "                 net_depth=None,\n",
    "                 pool_kernel_size=2,\n",
    "                 channel_mult=1,\n",
    "                 conv_per_layer=1,\n",
    "                 half_res=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            shape: Input shape. e.g. (192, 192, 192)\n",
    "            in_channels: Number of input features.\n",
    "            net_channels: Unet convolutional features. Can be specified via a list of lists with\n",
    "                the form [[encoder feats], [decoder feats]], or as a single integer. \n",
    "                If None (default), the unet features are defined by the default config described in \n",
    "                the class documentation.\n",
    "            net_depth: Number of levels in unet. Only used when net_channels is an integer. \n",
    "                Default is None.\n",
    "            pool_kernel_size: Maxpool layer kernel size\n",
    "            channel_mult: Per-level feature multiplier. Only used when net_channels is an integer. \n",
    "                Default is 1.\n",
    "            conv_per_layer: Number of convolutions per unet level. Default is 1.\n",
    "            half_res: Skip the last decoder upsampling. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # ensure correct dimensionality\n",
    "        ndims = len(shape)\n",
    "        assert ndims in [1, 2, 3], 'ndims should be one of 1, 2, or 3. found: %d' % ndims\n",
    "\n",
    "        # cache some parameters\n",
    "        self.half_res = half_res\n",
    "\n",
    "        # default encoder and decoder layer features if nothing provided\n",
    "        if net_channels is None:\n",
    "            net_channels = default_unet_features()\n",
    "\n",
    "        # build feature list automatically\n",
    "        if isinstance(net_channels, int):\n",
    "            if net_depth is None:\n",
    "                raise ValueError('must provide unet net_depth if net_channels is an integer')\n",
    "            feats = np.round(net_channels * channel_mult ** np.arange(net_depth)).astype(int)\n",
    "            net_channels = [\n",
    "                np.repeat(feats[:-1], conv_per_layer),\n",
    "                np.repeat(np.flip(feats), conv_per_layer)\n",
    "            ]\n",
    "        elif net_depth is not None:\n",
    "            raise ValueError('cannot use net_depth if net_channels is not an integer')\n",
    "\n",
    "        # extract any surplus (full resolution) decoder convolutions\n",
    "        encoder_channels, decoder_channels = net_channels\n",
    "        pivot = len(encoder_channels)\n",
    "        final_convs = decoder_channels[pivot:]\n",
    "        decoder_channels = decoder_channels[:pivot]\n",
    "        self.net_depth = int(pivot / conv_per_layer) + 1\n",
    "\n",
    "        if isinstance(pool_kernel_size, int):\n",
    "            pool_kernel_size = [pool_kernel_size] * self.net_depth\n",
    "\n",
    "        # cache downsampling / upsampling operations\n",
    "        MaxPooling = getattr(nn, 'MaxPool%dd' % ndims)\n",
    "        self.pooling = [MaxPooling(s) for s in pool_kernel_size]\n",
    "        self.upsampling = [nn.Upsample(scale_factor=s, mode='nearest') for s in pool_kernel_size]\n",
    "\n",
    "        # configure encoder (down-sampling path)\n",
    "        prev_nf = in_channels\n",
    "        encoder_nfs = [prev_nf]\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for level in range(self.net_depth - 1):\n",
    "            convs = nn.ModuleList()\n",
    "            for conv in range(conv_per_layer):\n",
    "                nf = encoder_channels[level * conv_per_layer + conv]\n",
    "                convs.append(ConvBlock(ndims, prev_nf, nf))\n",
    "                prev_nf = nf\n",
    "            self.encoder.append(convs)\n",
    "            encoder_nfs.append(prev_nf)\n",
    "\n",
    "        # configure decoder (up-sampling path)\n",
    "        encoder_nfs = np.flip(encoder_nfs)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for level in range(self.net_depth - 1):\n",
    "            convs = nn.ModuleList()\n",
    "            for conv in range(conv_per_layer):\n",
    "                nf = decoder_channels[level * conv_per_layer + conv]\n",
    "                convs.append(ConvBlock(ndims, prev_nf, nf))\n",
    "                prev_nf = nf\n",
    "            self.decoder.append(convs)\n",
    "            if not half_res or level < (self.net_depth - 2):\n",
    "                prev_nf += encoder_nfs[level]\n",
    "\n",
    "        # now we take care of any remaining convolutions\n",
    "        self.remaining = nn.ModuleList()\n",
    "        for num, nf in enumerate(final_convs):\n",
    "            self.remaining.append(ConvBlock(ndims, prev_nf, nf))\n",
    "            prev_nf = nf\n",
    "\n",
    "        # cache final number of features\n",
    "        self.final_nf = prev_nf\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder forward pass\n",
    "        x_history = [x]\n",
    "        for level, convs in enumerate(self.encoder):\n",
    "            for conv in convs:\n",
    "                x = conv(x)\n",
    "            x_history.append(x)\n",
    "            #print(self.pooling[level])\n",
    "            x = self.pooling[level](x)\n",
    "\n",
    "        # decoder forward pass with upsampling and concatenation\n",
    "        for level, convs in enumerate(self.decoder):\n",
    "            for conv in convs:\n",
    "                x = conv(x)\n",
    "            if not self.half_res or level < (self.net_depth - 2):\n",
    "                x = self.upsampling[level](x)\n",
    "                x = torch.cat([x, x_history.pop()], dim=1)\n",
    "\n",
    "        # remaining convs at full resolution\n",
    "        for conv in self.remaining:\n",
    "            x = conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6a7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (encoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (main): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (remaining): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (main): Conv3d(48, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (activation): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (main): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (activation): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (main): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (activation): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=Unet((128,128,128),1).cuda()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6415ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 16, 128, 128, 128]             448\n",
      "         LeakyReLU-2    [-1, 16, 128, 128, 128]               0\n",
      "         ConvBlock-3    [-1, 16, 128, 128, 128]               0\n",
      "            Conv3d-4       [-1, 32, 64, 64, 64]          13,856\n",
      "         LeakyReLU-5       [-1, 32, 64, 64, 64]               0\n",
      "         ConvBlock-6       [-1, 32, 64, 64, 64]               0\n",
      "            Conv3d-7       [-1, 32, 32, 32, 32]          27,680\n",
      "         LeakyReLU-8       [-1, 32, 32, 32, 32]               0\n",
      "         ConvBlock-9       [-1, 32, 32, 32, 32]               0\n",
      "           Conv3d-10       [-1, 32, 16, 16, 16]          27,680\n",
      "        LeakyReLU-11       [-1, 32, 16, 16, 16]               0\n",
      "        ConvBlock-12       [-1, 32, 16, 16, 16]               0\n",
      "           Conv3d-13          [-1, 32, 8, 8, 8]          27,680\n",
      "        LeakyReLU-14          [-1, 32, 8, 8, 8]               0\n",
      "        ConvBlock-15          [-1, 32, 8, 8, 8]               0\n",
      "           Conv3d-16       [-1, 32, 16, 16, 16]          55,328\n",
      "        LeakyReLU-17       [-1, 32, 16, 16, 16]               0\n",
      "        ConvBlock-18       [-1, 32, 16, 16, 16]               0\n",
      "           Conv3d-19       [-1, 32, 32, 32, 32]          55,328\n",
      "        LeakyReLU-20       [-1, 32, 32, 32, 32]               0\n",
      "        ConvBlock-21       [-1, 32, 32, 32, 32]               0\n",
      "           Conv3d-22       [-1, 32, 64, 64, 64]          55,328\n",
      "        LeakyReLU-23       [-1, 32, 64, 64, 64]               0\n",
      "        ConvBlock-24       [-1, 32, 64, 64, 64]               0\n",
      "           Conv3d-25    [-1, 32, 128, 128, 128]          41,504\n",
      "        LeakyReLU-26    [-1, 32, 128, 128, 128]               0\n",
      "        ConvBlock-27    [-1, 32, 128, 128, 128]               0\n",
      "           Conv3d-28    [-1, 16, 128, 128, 128]          13,840\n",
      "        LeakyReLU-29    [-1, 16, 128, 128, 128]               0\n",
      "        ConvBlock-30    [-1, 16, 128, 128, 128]               0\n",
      "           Conv3d-31    [-1, 16, 128, 128, 128]           6,928\n",
      "        LeakyReLU-32    [-1, 16, 128, 128, 128]               0\n",
      "        ConvBlock-33    [-1, 16, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 325,600\n",
      "Trainable params: 325,600\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 4278.38\n",
      "Params size (MB): 1.24\n",
      "Estimated Total Size (MB): 4287.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,128,128,128))\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fa3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]             896\n",
      "         LeakyReLU-2    [-1, 32, 128, 128, 128]               0\n",
      "         ConvBlock-3    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-4       [-1, 64, 64, 64, 64]          55,360\n",
      "         LeakyReLU-5       [-1, 64, 64, 64, 64]               0\n",
      "         ConvBlock-6       [-1, 64, 64, 64, 64]               0\n",
      "            Conv3d-7      [-1, 128, 32, 32, 32]         221,312\n",
      "         LeakyReLU-8      [-1, 128, 32, 32, 32]               0\n",
      "         ConvBlock-9      [-1, 128, 32, 32, 32]               0\n",
      "           Conv3d-10      [-1, 256, 16, 16, 16]         884,992\n",
      "        LeakyReLU-11      [-1, 256, 16, 16, 16]               0\n",
      "        ConvBlock-12      [-1, 256, 16, 16, 16]               0\n",
      "           Conv3d-13         [-1, 512, 8, 8, 8]       3,539,456\n",
      "        LeakyReLU-14         [-1, 512, 8, 8, 8]               0\n",
      "        ConvBlock-15         [-1, 512, 8, 8, 8]               0\n",
      "           Conv3d-16      [-1, 256, 16, 16, 16]       5,308,672\n",
      "        LeakyReLU-17      [-1, 256, 16, 16, 16]               0\n",
      "        ConvBlock-18      [-1, 256, 16, 16, 16]               0\n",
      "           Conv3d-19      [-1, 128, 32, 32, 32]       1,327,232\n",
      "        LeakyReLU-20      [-1, 128, 32, 32, 32]               0\n",
      "        ConvBlock-21      [-1, 128, 32, 32, 32]               0\n",
      "           Conv3d-22       [-1, 64, 64, 64, 64]         331,840\n",
      "        LeakyReLU-23       [-1, 64, 64, 64, 64]               0\n",
      "        ConvBlock-24       [-1, 64, 64, 64, 64]               0\n",
      "           Conv3d-25    [-1, 32, 128, 128, 128]          82,976\n",
      "        LeakyReLU-26    [-1, 32, 128, 128, 128]               0\n",
      "        ConvBlock-27    [-1, 32, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 11,752,736\n",
      "Trainable params: 11,752,736\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 4086.00\n",
      "Params size (MB): 44.83\n",
      "Estimated Total Size (MB): 4138.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=Unet((192,192,192),1,32,5,2,2).cuda()\n",
    "summary(model,(1,128,128,128))\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4475d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58a476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import inspect\n",
    "import functools\n",
    "\n",
    "\n",
    "def store_config_args(func):\n",
    "    \"\"\"\n",
    "    Class-method decorator that saves every argument provided to the\n",
    "    function as a dictionary in 'self.config'. This is used to assist\n",
    "    model loading - see LoadableModel.\n",
    "    \"\"\"\n",
    "\n",
    "    attrs, varargs, varkw, defaults = inspect.getargspec(func)\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        self.config = {}\n",
    "\n",
    "        # first save the default values\n",
    "        if defaults:\n",
    "            for attr, val in zip(reversed(attrs), reversed(defaults)):\n",
    "                self.config[attr] = val\n",
    "\n",
    "        # next handle positional args\n",
    "        for attr, val in zip(attrs[1:], args):\n",
    "            self.config[attr] = val\n",
    "\n",
    "        # lastly handle keyword args\n",
    "        if kwargs:\n",
    "            for attr, val in kwargs.items():\n",
    "                self.config[attr] = val\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class LoadableModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for easy pytorch model loading without having to manually\n",
    "    specify the architecture configuration at load time.\n",
    "\n",
    "    We can cache the arguments used to the construct the initial network, so that\n",
    "    we can construct the exact same network when loading from file. The arguments\n",
    "    provided to __init__ are automatically saved into the object (in self.config)\n",
    "    if the __init__ method is decorated with the @store_config_args utility.\n",
    "    \"\"\"\n",
    "\n",
    "    # this constructor just functions as a check to make sure that every\n",
    "    # LoadableModel subclass has provided an internal config parameter\n",
    "    # either manually or via store_config_args\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if not hasattr(self, 'config'):\n",
    "            raise RuntimeError('models that inherit from LoadableModel must decorate the '\n",
    "                               'constructor with @store_config_args')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model configuration and weights to a pytorch file.\n",
    "        \"\"\"\n",
    "        # don't save the transformer_grid buffers - see SpatialTransformer doc for more info\n",
    "        sd = self.state_dict().copy()\n",
    "        grid_buffers = [key for key in sd.keys() if key.endswith('.grid')]\n",
    "        for key in grid_buffers:\n",
    "            sd.pop(key)\n",
    "        torch.save({'config': self.config, 'model_state': sd}, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, device):\n",
    "        \"\"\"\n",
    "        Load a python model configuration and weights.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device(device))\n",
    "        model = cls(**checkpoint['config'])\n",
    "        model.load_state_dict(checkpoint['model_state'], strict=False)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fb05a",
   "metadata": {},
   "source": [
    "## STN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    N-D Spatial Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, mode='bilinear'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # create sampling grid\n",
    "        vectors = [torch.arange(0, s) for s in size]\n",
    "        grids = torch.meshgrid(vectors)\n",
    "        grid = torch.stack(grids)\n",
    "        grid = torch.unsqueeze(grid, 0)\n",
    "        grid = grid.type(torch.FloatTensor)\n",
    "\n",
    "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
    "        # adds it to the state dict. this is annoying since everything in the state dict\n",
    "        # is included when saving weights to disk, so the model files are way bigger\n",
    "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
    "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
    "        self.register_buffer('grid', grid)\n",
    "\n",
    "    def forward(self, src, flow):\n",
    "        # new locations\n",
    "        new_locs = self.grid + flow\n",
    "        shape = flow.shape[2:]\n",
    "\n",
    "        # need to normalize grid values to [-1, 1] for resampler\n",
    "        for i in range(len(shape)):\n",
    "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
    "\n",
    "        # move channels dim to last position\n",
    "        # also not sure why, but the channels need to be reversed\n",
    "        if len(shape) == 2:\n",
    "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
    "            new_locs = new_locs[..., [1, 0]]\n",
    "        elif len(shape) == 3:\n",
    "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
    "            new_locs = new_locs[..., [2, 1, 0]]\n",
    "\n",
    "        return nnf.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n",
    "\n",
    "\n",
    "class VecInt(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrates a vector field via scaling and squaring.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inshape, nsteps):\n",
    "        super().__init__()\n",
    "\n",
    "        assert nsteps >= 0, 'nsteps should be >= 0, found: %d' % nsteps\n",
    "        self.nsteps = nsteps\n",
    "        self.scale = 1.0 / (2 ** self.nsteps)\n",
    "        self.transformer = SpatialTransformer(inshape)\n",
    "\n",
    "    def forward(self, vec):\n",
    "        vec = vec * self.scale\n",
    "        for _ in range(self.nsteps):\n",
    "            vec = vec + self.transformer(vec, vec)\n",
    "        return vec\n",
    "\n",
    "\n",
    "class ResizeTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    Resize a transform, which involves resizing the vector field *and* rescaling it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vel_resize, ndims):\n",
    "        super().__init__()\n",
    "        self.factor = 1.0 / vel_resize\n",
    "        self.mode = 'linear'\n",
    "        if ndims == 2:\n",
    "            self.mode = 'bi' + self.mode\n",
    "        elif ndims == 3:\n",
    "            self.mode = 'tri' + self.mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.factor < 1:\n",
    "            # resize first to save memory\n",
    "            x = nnf.interpolate(x, align_corners=True, scale_factor=self.factor, mode=self.mode)\n",
    "            x = self.factor * x\n",
    "\n",
    "        elif self.factor > 1:\n",
    "            # multiply first to save memory\n",
    "            x = self.factor * x\n",
    "            x = nnf.interpolate(x, align_corners=True, scale_factor=self.factor, mode=self.mode)\n",
    "\n",
    "        # don't do anything if resize is 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36598872",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5220d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class NCC:\n",
    "    \"\"\"\n",
    "    Local (over window) normalized cross correlation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, win=None):\n",
    "        self.win = win\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "\n",
    "        Ii = y_true\n",
    "        Ji = y_pred\n",
    "\n",
    "        # get dimension of volume\n",
    "        # assumes Ii, Ji are sized [batch_size, *vol_shape, nb_feats]\n",
    "        ndims = len(list(Ii.size())) - 2\n",
    "        assert ndims in [1, 2, 3], \"volumes should be 1 to 3 dimensions. found: %d\" % ndims\n",
    "\n",
    "        # set window size\n",
    "        win = [9] * ndims if self.win is None else self.win\n",
    "\n",
    "        # compute filters\n",
    "        sum_filt = torch.ones([1, 1, *win]).to(\"cuda\")\n",
    "\n",
    "        pad_no = math.floor(win[0] / 2)\n",
    "\n",
    "        if ndims == 1:\n",
    "            stride = (1)\n",
    "            padding = (pad_no)\n",
    "        elif ndims == 2:\n",
    "            stride = (1, 1)\n",
    "            padding = (pad_no, pad_no)\n",
    "        else:\n",
    "            stride = (1, 1, 1)\n",
    "            padding = (pad_no, pad_no, pad_no)\n",
    "\n",
    "        # get convolution function\n",
    "        conv_fn = getattr(F, 'conv%dd' % ndims)\n",
    "\n",
    "        # compute CC squares\n",
    "        I2 = Ii * Ii\n",
    "        J2 = Ji * Ji\n",
    "        IJ = Ii * Ji\n",
    "\n",
    "        I_sum = conv_fn(Ii, sum_filt, stride=stride, padding=padding)\n",
    "        J_sum = conv_fn(Ji, sum_filt, stride=stride, padding=padding)\n",
    "        I2_sum = conv_fn(I2, sum_filt, stride=stride, padding=padding)\n",
    "        J2_sum = conv_fn(J2, sum_filt, stride=stride, padding=padding)\n",
    "        IJ_sum = conv_fn(IJ, sum_filt, stride=stride, padding=padding)\n",
    "\n",
    "        win_size = np.prod(win)\n",
    "        u_I = I_sum / win_size\n",
    "        u_J = J_sum / win_size\n",
    "\n",
    "        cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size\n",
    "        I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size\n",
    "        J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size\n",
    "\n",
    "        cc = cross * cross / (I_var * J_var + 1e-5)\n",
    "\n",
    "        return -torch.mean(cc)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    \"\"\"\n",
    "    Mean squared error loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "class Dice:\n",
    "    \"\"\"\n",
    "    N-D dice for segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        ndims = len(list(y_pred.size())) - 2\n",
    "        vol_axes = list(range(2, ndims + 2))\n",
    "        top = 2 * (y_true * y_pred).sum(dim=vol_axes)\n",
    "        bottom = torch.clamp((y_true + y_pred).sum(dim=vol_axes), min=1e-5)\n",
    "        dice = torch.mean(top / bottom)\n",
    "        return -dice\n",
    "\n",
    "\n",
    "class Grad:\n",
    "    \"\"\"\n",
    "    N-D gradient loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty='l1', loss_mult=None):\n",
    "        self.penalty = penalty\n",
    "        self.loss_mult = loss_mult\n",
    "\n",
    "    def loss(self, _, y_pred):\n",
    "        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
    "        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
    "        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
    "\n",
    "        if self.penalty == 'l2':\n",
    "            dy = dy * dy\n",
    "            dx = dx * dx\n",
    "            dz = dz * dz\n",
    "\n",
    "        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
    "        grad = d / 3.0\n",
    "\n",
    "        if self.loss_mult is not None:\n",
    "            grad *= self.loss_mult\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41284fe3",
   "metadata": {},
   "source": [
    "# VoxelMorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd0d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class VxmDense(LoadableModel):\n",
    "    \"\"\"\n",
    "    VoxelMorph network for (unsupervised) nonlinear registration between two images.\n",
    "    \"\"\"\n",
    "\n",
    "    @store_config_args\n",
    "    def __init__(self,\n",
    "                 inshape,\n",
    "                 nb_unet_features=None,\n",
    "                 nb_unet_levels=None,\n",
    "                 unet_feat_mult=1,\n",
    "                 nb_unet_conv_per_level=1,\n",
    "                 int_steps=7,\n",
    "                 int_downsize=2,\n",
    "                 bidir=False,\n",
    "                 use_probs=False,\n",
    "                 src_feats=1,\n",
    "                 trg_feats=1,\n",
    "                 unet_half_res=False):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "            inshape: Input shape. e.g. (192, 192, 192)\n",
    "            nb_unet_features: Unet convolutional features. Can be specified via a list of lists with\n",
    "                the form [[encoder feats], [decoder feats]], or as a single integer. \n",
    "                If None (default), the unet features are defined by the default config described in \n",
    "                the unet class documentation.\n",
    "            nb_unet_levels: Number of levels in unet. Only used when nb_features is an integer. \n",
    "                Default is None.\n",
    "            unet_feat_mult: Per-level feature multiplier. Only used when nb_features is an integer. \n",
    "                Default is 1.\n",
    "            nb_unet_conv_per_level: Number of convolutions per unet level. Default is 1.\n",
    "            int_steps: Number of flow integration steps. The warp is non-diffeomorphic when this \n",
    "                value is 0.\n",
    "            int_downsize: Integer specifying the flow downsample factor for vector integration. \n",
    "                The flow field is not downsampled when this value is 1.\n",
    "            bidir: Enable bidirectional cost function. Default is False.\n",
    "            use_probs: Use probabilities in flow field. Default is False.\n",
    "            src_feats: Number of source image features. Default is 1.\n",
    "            trg_feats: Number of target image features. Default is 1.\n",
    "            unet_half_res: Skip the last unet decoder upsampling. Requires that int_downsize=2. \n",
    "                Default is False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # internal flag indicating whether to return flow or integrated warp during inference\n",
    "        self.training = True\n",
    "\n",
    "        # ensure correct dimensionality\n",
    "        ndims = len(inshape)\n",
    "        assert ndims in [1, 2, 3], 'ndims should be one of 1, 2, or 3. found: %d' % ndims\n",
    "\n",
    "        # configure core unet model\n",
    "        self.unet_model = Unet(\n",
    "            shape=inshape,\n",
    "            in_channels=(src_feats + trg_feats),\n",
    "            net_channels=nb_unet_features,\n",
    "            net_depth=nb_unet_levels,\n",
    "            channel_mult=unet_feat_mult,\n",
    "            conv_per_layer=nb_unet_conv_per_level,\n",
    "            half_res=unet_half_res,\n",
    "        )\n",
    "\n",
    "        # configure unet to flow field layer\n",
    "        Conv = getattr(nn, 'Conv%dd' % ndims)\n",
    "        self.flow = Conv(self.unet_model.final_nf, ndims, kernel_size=3, padding=1)\n",
    "\n",
    "        # init flow layer with small weights and bias\n",
    "        self.flow.weight = nn.Parameter(Normal(0, 1e-5).sample(self.flow.weight.shape))\n",
    "        self.flow.bias = nn.Parameter(torch.zeros(self.flow.bias.shape))\n",
    "\n",
    "        # probabilities are not supported in pytorch\n",
    "        if use_probs:\n",
    "            raise NotImplementedError(\n",
    "                'Flow variance has not been implemented in pytorch - set use_probs to False')\n",
    "\n",
    "        # configure optional resize layers (downsize)\n",
    "        if not unet_half_res and int_steps > 0 and int_downsize > 1:\n",
    "            self.resize = ResizeTransform(int_downsize, ndims)\n",
    "        else:\n",
    "            self.resize = None\n",
    "\n",
    "        # resize to full res\n",
    "        if int_steps > 0 and int_downsize > 1:\n",
    "            self.fullsize = ResizeTransform(1 / int_downsize, ndims)\n",
    "        else:\n",
    "            self.fullsize = None\n",
    "\n",
    "        # configure bidirectional training\n",
    "        self.bidir = bidir\n",
    "\n",
    "        # configure optional integration layer for diffeomorphic warp\n",
    "        down_shape = [int(dim / int_downsize) for dim in inshape]\n",
    "        self.integrate = VecInt(down_shape, int_steps) if int_steps > 0 else None\n",
    "\n",
    "        # configure transformer\n",
    "        self.transformer = SpatialTransformer(inshape)\n",
    "\n",
    "    def forward(self, source, target, registration=False):\n",
    "        '''\n",
    "        Parameters:\n",
    "            source: Source image tensor.\n",
    "            target: Target image tensor.\n",
    "            registration: Return transformed image and flow. Default is False.\n",
    "        '''\n",
    "\n",
    "        # concatenate inputs and propagate unet\n",
    "        print(source.shape, target.shape)\n",
    "        x = torch.cat([source, target], dim=1)\n",
    "        x = self.unet_model(x)\n",
    "\n",
    "        # transform into flow field\n",
    "        flow_field = self.flow(x)\n",
    "\n",
    "        # resize flow for integration\n",
    "        pos_flow = flow_field\n",
    "        if self.resize:\n",
    "            pos_flow = self.resize(pos_flow)\n",
    "\n",
    "        preint_flow = pos_flow\n",
    "\n",
    "        # negate flow for bidirectional model\n",
    "        neg_flow = -pos_flow if self.bidir else None\n",
    "\n",
    "        # integrate to produce diffeomorphic warp\n",
    "        if self.integrate:\n",
    "            pos_flow = self.integrate(pos_flow)\n",
    "            neg_flow = self.integrate(neg_flow) if self.bidir else None\n",
    "\n",
    "            # resize to final resolution\n",
    "            if self.fullsize:\n",
    "                pos_flow = self.fullsize(pos_flow)\n",
    "                neg_flow = self.fullsize(neg_flow) if self.bidir else None\n",
    "\n",
    "        # warp image with flow field\n",
    "        y_source = self.transformer(source, pos_flow)\n",
    "        y_target = self.transformer(target, neg_flow) if self.bidir else None\n",
    "\n",
    "        # return non-integrated flow field if training\n",
    "        if not registration:\n",
    "            return (y_source, y_target, preint_flow) if self.bidir else (y_source, preint_flow)\n",
    "        else:\n",
    "            return y_source, pos_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34061627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 128, 128]) torch.Size([2, 128, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 64, 3, 3, 3], expected input[1, 32, 48, 16, 16] to have 64 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44028\\1511223444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVxmDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# make a forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# remove these hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44028\\2770084866.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, source, target, registration)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munet_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# transform into flow field\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44028\\4198808899.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf_res\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_depth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44028\\4198808899.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    601\u001b[0m             )\n\u001b[0;32m    602\u001b[0m         return F.conv3d(\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m         )\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 64, 3, 3, 3], expected input[1, 32, 48, 16, 16] to have 64 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "model = VxmDense((128,128,128)).cuda()\n",
    "#print(model)\n",
    "summary(model,[(1,128,128,128),(1,128,128,128)])\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5bf56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01652d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
